#!/usr/bin/env python3
"""
CSV Data Importer for ManaXC
Imports data from CSV files generated by athletic_net_scraper_v2.py
with validation, preview mode, and batch processing.
"""

import sys
import csv
import json
import os
import shutil
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
from supabase import create_client, Client

# Load environment variables
from dotenv import load_dotenv
load_dotenv(os.path.join(os.path.dirname(__file__), '../../website/.env.local'))

SUPABASE_URL = os.getenv('NEXT_PUBLIC_SUPABASE_URL')
SUPABASE_KEY = os.getenv('NEXT_PUBLIC_SUPABASE_ANON_KEY')

if not SUPABASE_URL or not SUPABASE_KEY:
    print("‚ùå Error: Missing Supabase credentials")
    sys.exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)


# ==============================================================================
# VALIDATION FUNCTIONS
# ==============================================================================

@dataclass
class ValidationResult:
    """Result of validation check"""
    is_valid: bool
    errors: List[str]
    warnings: List[str]


def validate_result(result: Dict, season_year: int) -> ValidationResult:
    """
    Validate a single result record.

    Critical validations (must pass):
    - Time: > 0 cs (must be positive)
    - Required fields: athlete_name, time_cs, place_overall, grade

    Warning validations (log but allow):
    - Grade out of range: < 9 or > 13

    Note: Time limits removed to support various distances (XC, 10K, marathon, etc.)
    Times should be validated against course distance and slow walking pace if needed

    Args:
        result: Result dictionary from CSV
        season_year: Season year for context

    Returns:
        ValidationResult with is_valid, errors, and warnings
    """
    errors = []
    warnings = []

    # Critical: Required fields
    if not result.get('athlete_name'):
        errors.append("Missing athlete_name")

    if not result.get('time_cs'):
        errors.append("Missing time_cs")
    else:
        time_cs = int(result['time_cs'])

        # Critical: Time must be positive
        if time_cs <= 0:
            errors.append(f"Invalid time: {time_cs} cs (must be positive)")

        # No upper/lower bound warnings - support all race distances

    if not result.get('place_overall'):
        errors.append("Missing place_overall")

    if not result.get('grade'):
        errors.append("Missing grade")
    else:
        grade = int(result['grade'])
        if grade < 9 or grade > 13:
            warnings.append(f"Grade out of range: {grade} (expected 9-13)")

    return ValidationResult(
        is_valid=len(errors) == 0,
        errors=errors,
        warnings=warnings
    )


def validate_athlete(athlete: Dict, season_year: int) -> ValidationResult:
    """Validate athlete record"""
    errors = []
    warnings = []

    if not athlete.get('name'):
        errors.append("Missing athlete name")

    if not athlete.get('first_name') or not athlete.get('last_name'):
        warnings.append("Missing first or last name")

    if not athlete.get('grad_year'):
        errors.append("Missing grad_year")
    else:
        grad_year = int(athlete['grad_year'])
        expected_min = season_year - 1
        expected_max = season_year + 4
        if grad_year < expected_min or grad_year > expected_max:
            warnings.append(f"Unusual grad_year: {grad_year} (expected {expected_min}-{expected_max})")

    return ValidationResult(
        is_valid=len(errors) == 0,
        errors=errors,
        warnings=warnings
    )


# ==============================================================================
# CSV READING FUNCTIONS
# ==============================================================================

def read_csv_folder(folder_path: str) -> Dict:
    """
    Read all CSV files from a scrape folder.

    Args:
        folder_path: Path to folder containing CSV files

    Returns:
        Dictionary with all data:
        {
            'venues': [...],
            'courses': [...],
            'schools': [...],
            'athletes': [...],
            'meets': [...],
            'races': [...],
            'results': [...],
            'metadata': {...}
        }
    """
    data = {
        'venues': [],
        'courses': [],
        'schools': [],
        'athletes': [],
        'meets': [],
        'races': [],
        'results': [],
        'metadata': {}
    }

    # Read metadata
    metadata_file = os.path.join(folder_path, 'metadata.json')
    if os.path.exists(metadata_file):
        with open(metadata_file, 'r', encoding='utf-8') as f:
            data['metadata'] = json.load(f)

    # Read each CSV file
    csv_files = {
        'venues': 'venues.csv',
        'courses': 'courses.csv',
        'schools': 'schools.csv',
        'athletes': 'athletes.csv',
        'meets': 'meets.csv',
        'races': 'races.csv',
        'results': 'results.csv'
    }

    for key, filename in csv_files.items():
        filepath = os.path.join(folder_path, filename)
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                data[key] = list(reader)

    return data


# ==============================================================================
# IMPORT FUNCTIONS
# ==============================================================================

def import_csv_folder(
    folder_path: str,
    preview_only: bool = False,
    target_school_name: Optional[str] = None
) -> Dict:
    """
    Import data from CSV folder to Supabase.

    Args:
        folder_path: Path to folder with CSV files
        preview_only: If True, validate but don't import
        target_school_name: Optional school name filter

    Returns:
        Dictionary with import statistics and validation results
    """
    print(f"\nüì• {'Previewing' if preview_only else 'Importing'} data from {folder_path}")
    print("=" * 60)

    # Read CSV files
    data = read_csv_folder(folder_path)
    metadata = data['metadata']
    season_year = metadata.get('season_year', datetime.now().year)

    # Statistics
    stats = {
        'venues_created': 0,
        'courses_created': 0,
        'schools_created': 0,
        'athletes_created': 0,
        'meets_created': 0,
        'races_created': 0,
        'results_inserted': 0,
        'validation_errors': [],
        'validation_warnings': [],
        'skipped_results': 0,
        'skipped_already_exists': 0,
        'skipped_missing_athlete': 0,
        'skipped_missing_race': 0
    }

    # Validate all results
    print(f"\nüîç Validating {len(data['results'])} results...")
    for idx, result in enumerate(data['results']):
        validation = validate_result(result, season_year)

        if not validation.is_valid:
            stats['validation_errors'].extend([f"Result {idx+1}: {e}" for e in validation.errors])

        stats['validation_warnings'].extend([f"Result {idx+1}: {w}" for w in validation.warnings])

    # Validate all athletes
    print(f"üîç Validating {len(data['athletes'])} athletes...")
    for idx, athlete in enumerate(data['athletes']):
        validation = validate_athlete(athlete, season_year)

        if not validation.is_valid:
            stats['validation_errors'].extend([f"Athlete {idx+1}: {e}" for e in validation.errors])

        stats['validation_warnings'].extend([f"Athlete {idx+1}: {w}" for w in validation.warnings])

    # Print validation summary
    print(f"\nüìä Validation Summary:")
    print(f"  Errors: {len(stats['validation_errors'])}")
    print(f"  Warnings: {len(stats['validation_warnings'])}")

    if stats['validation_errors']:
        print(f"\n‚ùå Validation Errors:")
        for error in stats['validation_errors'][:10]:  # Show first 10
            print(f"  - {error}")
        if len(stats['validation_errors']) > 10:
            print(f"  ... and {len(stats['validation_errors']) - 10} more")

    if stats['validation_warnings']:
        print(f"\n‚ö†Ô∏è  Validation Warnings:")
        for warning in stats['validation_warnings'][:5]:  # Show first 5
            print(f"  - {warning}")
        if len(stats['validation_warnings']) > 5:
            print(f"  ... and {len(stats['validation_warnings']) - 5} more")

    # Stop if preview mode or validation errors
    if preview_only:
        print(f"\n‚úÖ Preview complete - no data imported")
        return stats

    if stats['validation_errors']:
        print(f"\n‚ùå Cannot import - fix validation errors first")
        return stats

    # ========== STAGE 1: IMPORT VENUES ==========
    print(f"\nüìç Stage 1/7: Importing Venues")
    print(f"  [DEBUG] Processing {len(data['venues'])} venues...")
    venue_id_map = {}  # Map venue_name to database ID

    for venue in data['venues']:
        athletic_net_id = venue.get('athletic_net_id', '').strip()
        print(f"  [DEBUG] Processing venue: {venue['name']}, athletic_net_id: '{athletic_net_id}'")

        # Check if exists by athletic_net_id (if it's not empty) or by name
        if athletic_net_id:
            existing = supabase.table('venues').select('id').eq('athletic_net_id', athletic_net_id).execute()
        else:
            existing = supabase.table('venues').select('id').eq('name', venue['name']).execute()

        print(f"  [DEBUG] Existing check returned: {len(existing.data) if existing.data else 0} results")

        if existing.data:
            venue_id_map[venue['name']] = existing.data[0]['id']
            print(f"  [DEBUG] Venue exists, using ID: {existing.data[0]['id']}")
        else:
            print(f"  [DEBUG] Creating new venue...")
            venue_data = {
                'name': venue['name'],
                'city': venue.get('city', ''),
                'state': venue.get('state', ''),
                'athletic_net_id': athletic_net_id if athletic_net_id else None,
                'notes': venue.get('notes', '')
            }
            try:
                response = supabase.table('venues').insert(venue_data).execute()
                venue_id_map[venue['name']] = response.data[0]['id']
                stats['venues_created'] += 1
                print(f"  ‚úÖ Created venue: {venue['name']}")
            except Exception as e:
                print(f"  ‚ùå ERROR creating venue {venue['name']}: {e}")
                continue

    print(f"  [DEBUG] venue_id_map has {len(venue_id_map)} entries")

    # ========== STAGE 2: IMPORT COURSES ==========
    print(f"\nüèÉ Stage 2/7: Importing Courses")
    print(f"  [DEBUG] Processing {len(data['courses'])} courses...")
    print(f"  [DEBUG] venue_id_map keys: {list(venue_id_map.keys())}")
    course_id_map = {}  # Map course_name to database ID

    for course in data['courses']:
        print(f"  [DEBUG] Processing course: {course['name']}, venue: {course['venue_name']}")
        # Get venue ID
        venue_id = venue_id_map.get(course['venue_name'])
        print(f"  [DEBUG] Looked up venue_id: {venue_id}")
        if not venue_id:
            print(f"  ‚ö†Ô∏è  Skipping course {course['name']} - venue not found")
            continue

        athletic_net_id = course.get('athletic_net_id', '').strip()
        print(f"  [DEBUG] Course athletic_net_id: '{athletic_net_id}'")

        # Check if exists by athletic_net_id (if it's not empty) or by name+venue
        if athletic_net_id:
            existing = supabase.table('courses').select('id').eq('athletic_net_id', athletic_net_id).execute()
        else:
            print(f"  [DEBUG] Checking for existing course by name+venue...")
            existing = supabase.table('courses').select('id').eq('name', course['name']).eq('venue_id', venue_id).execute()

        print(f"  [DEBUG] Existing check returned: {len(existing.data) if existing.data else 0} results")

        if existing.data:
            course_id_map[course['name']] = existing.data[0]['id']
            print(f"  [DEBUG] Course exists, using ID: {existing.data[0]['id']}")
        else:
            print(f"  [DEBUG] Creating new course...")
            course_data = {
                'name': course['name'],
                'venue_id': venue_id,
                'distance_meters': int(course['distance_meters']),
                'difficulty_rating': float(course.get('difficulty_rating', 5.0)),
                'athletic_net_id': athletic_net_id if athletic_net_id else None,
            }
            try:
                response = supabase.table('courses').insert(course_data).execute()
                course_id_map[course['name']] = response.data[0]['id']
                stats['courses_created'] += 1
                print(f"  ‚úÖ Created course: {course['name']}")
            except Exception as e:
                print(f"  ‚ùå ERROR creating course {course['name']}: {e}")
                continue

    print(f"  [DEBUG] course_id_map has {len(course_id_map)} entries")

    # ========== STAGE 3: IMPORT SCHOOLS ==========
    print(f"\nüè´ Stage 3/7: Importing Schools")
    school_id_map = {}  # Map athletic_net_id to database ID

    for school in data['schools']:
        athletic_net_id = school.get('athletic_net_id', '').strip()
        print(f"  [DEBUG] Processing school: {school['name']}, athletic_net_id: '{athletic_net_id}'")

        # Check if exists by athletic_net_id
        if athletic_net_id:
            print(f"  [DEBUG] Checking if school exists by athletic_net_id: {athletic_net_id}")
            existing = supabase.table('schools').select('id').eq('athletic_net_id', athletic_net_id).execute()
        else:
            print(f"  [DEBUG] No athletic_net_id, checking by name: {school['name']}")
            existing = supabase.table('schools').select('id').eq('name', school['name']).execute()

        print(f"  [DEBUG] Existing check returned: {len(existing.data) if existing.data else 0} results")

        if existing.data:
            print(f"  [DEBUG] School exists, using ID: {existing.data[0]['id']}")
            school_id_map[athletic_net_id] = existing.data[0]['id']
        else:
            print(f"  [DEBUG] School doesn't exist, creating new...")
            school_data = {
                'name': school['name'],
                'short_name': school.get('short_name', school['name']),
                'city': school.get('city', ''),
                'state': school.get('state', ''),
                'athletic_net_id': athletic_net_id if athletic_net_id else None,
            }
            print(f"  [DEBUG] Inserting school data: {school_data}")
            try:
                response = supabase.table('schools').insert(school_data).execute()
                print(f"  [DEBUG] Insert response: {response.data}")
                school_id_map[athletic_net_id] = response.data[0]['id']
                stats['schools_created'] += 1
                print(f"  ‚úÖ Created school: {school['name']}")
            except Exception as e:
                print(f"  ‚ùå ERROR creating school {school['name']}: {e}")
                continue

    # ========== STAGE 4: IMPORT ATHLETES ==========
    print(f"\nüë• Stage 4/7: Importing Athletes")
    print(f"  Processing {len(data['athletes'])} athletes...")
    print(f"  [DEBUG] school_id_map has {len(school_id_map)} entries: {list(school_id_map.keys())[:5]}")
    athlete_map = {}  # Map (name, school_id) to database ID

    for i, athlete in enumerate(data['athletes'], 1):
        if i % 100 == 0:
            print(f"  Progress: {i}/{len(data['athletes'])} athletes processed...")
        if i <= 3:  # Debug first 3 athletes
            print(f"  [DEBUG] Athlete {i}: {athlete['name']}, school_id: {athlete['school_athletic_net_id']}")

        school_db_id = school_id_map.get(athlete['school_athletic_net_id'])
        if i <= 3:
            print(f"  [DEBUG] Looked up school_db_id: {school_db_id}")

        if not school_db_id:
            print(f"  ‚ö†Ô∏è  Skipping athlete {athlete['name']} - school not found (looked for: {athlete['school_athletic_net_id']})")
            continue

        # Check if exists
        existing = supabase.table('athletes').select('id').eq('first_name', athlete['first_name']).eq('last_name', athlete['last_name']).eq('school_id', school_db_id).execute()

        if i <= 3:
            print(f"  [DEBUG] Existing check returned: {len(existing.data) if existing.data else 0} results")

        if existing.data:
            athlete_map[(athlete['name'], athlete['school_athletic_net_id'])] = existing.data[0]['id']
            if i <= 3:
                print(f"  [DEBUG] Athlete exists, using ID: {existing.data[0]['id']}")
        else:
            if i <= 3:
                print(f"  [DEBUG] Creating new athlete...")
            # Keep gender as text 'M' or 'F' (schema has check constraint)
            athlete_data = {
                'name': athlete['name'],  # Combined name field in current schema
                'first_name': athlete['first_name'],
                'last_name': athlete['last_name'],
                'school_id': school_db_id,
                'grad_year': int(athlete['grad_year']),
                'gender': athlete['gender'],  # Keep as 'M' or 'F' text
                'athletic_net_id': athlete.get('athletic_net_id') or ''
            }
            try:
                response = supabase.table('athletes').insert(athlete_data).execute()
                athlete_map[(athlete['name'], athlete['school_athletic_net_id'])] = response.data[0]['id']
                stats['athletes_created'] += 1
                if i <= 3:
                    print(f"  [DEBUG] Created athlete ID: {response.data[0]['id']}")
            except Exception as e:
                print(f"  ‚ùå ERROR creating athlete {athlete['name']}: {e}")
                continue

    print(f"  ‚úÖ Created {stats['athletes_created']} athletes")
    print(f"  [DEBUG] athlete_map has {len(athlete_map)} entries")

    # ========== STAGE 5: IMPORT MEETS ==========
    print(f"\nüèÅ Stage 5/7: Importing Meets")
    print(f"  [DEBUG] Processing {len(data['meets'])} meets...")
    print(f"  [DEBUG] venue_id_map keys: {list(venue_id_map.keys())}")
    meet_id_map = {}  # Map athletic_net_id to database ID

    for meet in data['meets']:
        print(f"  [DEBUG] Processing meet: {meet['name']}, venue: {meet.get('venue_name')}")
        # Get venue ID (meets are at venues, not courses)
        # Use the venue from our venue_id_map
        venue_id = venue_id_map.get(meet.get('venue_name')) or venue_id_map.get('Unknown Venue')
        print(f"  [DEBUG] Looked up venue_id: {venue_id}")
        if not venue_id:
            print(f"  ‚ö†Ô∏è  Skipping meet {meet['name']} - no venue available")
            continue

        athletic_net_id = meet.get('athletic_net_id', '').strip()
        print(f"  [DEBUG] Meet athletic_net_id: '{athletic_net_id}'")

        # Check if exists by athletic_net_id (if not empty) or by name+date
        if athletic_net_id:
            existing = supabase.table('meets').select('id').eq('athletic_net_id', athletic_net_id).execute()
        else:
            existing = supabase.table('meets').select('id').eq('name', meet['name']).eq('meet_date', meet['meet_date']).execute()

        print(f"  [DEBUG] Existing check returned: {len(existing.data) if existing.data else 0} results")

        if existing.data:
            meet_id_map[athletic_net_id] = existing.data[0]['id']
            print(f"  [DEBUG] Meet exists, using ID: {existing.data[0]['id']}")
        else:
            print(f"  [DEBUG] Creating new meet...")
            meet_data = {
                'name': meet['name'],
                'meet_date': meet['meet_date'],
                'venue_id': venue_id,  # Changed from course_id to venue_id
                'season_year': int(meet['season_year']),
                'athletic_net_id': athletic_net_id if athletic_net_id else None,
            }
            try:
                response = supabase.table('meets').insert(meet_data).execute()
                meet_id_map[athletic_net_id] = response.data[0]['id']
                stats['meets_created'] += 1
                print(f"  ‚úÖ Created meet: {meet['name']}")
            except Exception as e:
                print(f"  ‚ùå ERROR creating meet {meet['name']}: {e}")
                continue

    print(f"  [DEBUG] meet_id_map has {len(meet_id_map)} entries: {list(meet_id_map.keys())}")

    # ========== STAGE 6: IMPORT RACES ==========
    print(f"\nüèÉ‚Äç‚ôÇÔ∏è Stage 6/7: Importing Races")
    print(f"  [DEBUG] Processing {len(data['races'])} races...")
    print(f"  [DEBUG] meet_id_map keys: {list(meet_id_map.keys())}")
    print(f"  [DEBUG] course_id_map has {len(course_id_map)} entries")
    race_id_map = {}  # Map athletic_net_race_id to database ID
    race_to_meet_map = {}  # Map athletic_net_race_id to meet_db_id

    for i, race in enumerate(data['races'], 1):
        if i <= 3:
            print(f"  [DEBUG] Race {i}: {race['name']}, meet_id: {race['meet_athletic_net_id']}")
        meet_db_id = meet_id_map.get(race['meet_athletic_net_id'])
        if i <= 3:
            print(f"  [DEBUG] Looked up meet_db_id: {meet_db_id}")
        if not meet_db_id:
            print(f"  ‚ö†Ô∏è  Skipping race {race['name']} - meet not found (looked for: {race['meet_athletic_net_id']})")
            continue

        # Get course_id - look up by course_name from race, or fall back to first course
        course_name_from_race = race.get('course_name', '').strip()
        if course_name_from_race and course_name_from_race in course_id_map:
            course_id = course_id_map[course_name_from_race]
            if i <= 3:
                print(f"  [DEBUG] Using course_id from race's course_name '{course_name_from_race}': {course_id}")
        else:
            # Fall back to first course (for backward compatibility with old CSVs)
            course_id = list(course_id_map.values())[0] if course_id_map else None
            if i <= 3:
                print(f"  [DEBUG] Using first available course_id (fallback): {course_id}")

        if not course_id:
            print(f"  ‚ö†Ô∏è  Skipping race - no course available")
            continue

        athletic_net_race_id = race.get('athletic_net_race_id', '').strip()
        if i <= 3:
            print(f"  [DEBUG] Race athletic_net_race_id: '{athletic_net_race_id}'")

        # Check if race already exists by athletic_net_race_id or by meet+name+gender
        if athletic_net_race_id:
            existing = supabase.table('races').select('id').eq('athletic_net_race_id', athletic_net_race_id).execute()
        else:
            existing = supabase.table('races').select('id').eq('meet_id', meet_db_id).eq('name', race['name']).eq('gender', race['gender']).execute()

        if i <= 3:
            print(f"  [DEBUG] Existing check returned: {len(existing.data) if existing.data else 0} results")

        if existing.data:
            race_id_map[athletic_net_race_id] = existing.data[0]['id']
            race_to_meet_map[athletic_net_race_id] = meet_db_id
            if i <= 3:
                print(f"  [DEBUG] Race exists, using ID: {existing.data[0]['id']}")
        else:
            if i <= 3:
                print(f"  [DEBUG] Creating new race...")
            # Keep gender as text 'M' or 'F' (schema has check constraint)
            race_data = {
                'meet_id': meet_db_id,
                'course_id': course_id,  # Races are on courses
                'name': race['name'],
                'gender': race['gender'],  # Keep as 'M' or 'F' text
                'distance_meters': int(race['distance_meters']),  # Required field
                'athletic_net_race_id': athletic_net_race_id if athletic_net_race_id else None
            }
            try:
                response = supabase.table('races').insert(race_data).execute()
                race_id_map[athletic_net_race_id] = response.data[0]['id']
                race_to_meet_map[athletic_net_race_id] = meet_db_id
                stats['races_created'] += 1
                if i <= 3:
                    print(f"  [DEBUG] Created race ID: {response.data[0]['id']}")
            except Exception as e:
                print(f"  ‚ùå ERROR creating race {race['name']}: {e}")
                continue

    print(f"  ‚úÖ Created {stats['races_created']} races")
    print(f"  [DEBUG] race_id_map has {len(race_id_map)} entries")

    # ========== STAGE 7: IMPORT RESULTS ==========
    print(f"\nüìä Stage 7/7: Importing Results")
    print(f"  [DEBUG] Processing {len(data['results'])} results...")
    print(f"  [DEBUG] race_id_map keys: {list(race_id_map.keys())[:5]}")
    print(f"  [DEBUG] race_to_meet_map keys: {list(race_to_meet_map.keys())[:5]}")
    print(f"  [DEBUG] athlete_map has {len(athlete_map)} entries")

    # Batch import results (100 at a time)
    BATCH_SIZE = 100
    results_to_import = []

    for i, result in enumerate(data['results'], 1):
        if i <= 3:
            print(f"  [DEBUG] Result {i}: {result['athlete_name']}, race_id: {result['athletic_net_race_id']}")

        race_db_id = race_id_map.get(result['athletic_net_race_id'])
        if i <= 3:
            print(f"  [DEBUG] Looked up race_db_id: {race_db_id}")
        if not race_db_id:
            if i <= 3:
                print(f"  ‚ö†Ô∏è  Skipping result {i} - race not found (looked for: {result['athletic_net_race_id']})")
            stats['skipped_results'] += 1
            stats['skipped_missing_race'] += 1
            continue

        athlete_key = (result['athlete_name'], result['athlete_school_id'])
        if i <= 3:
            print(f"  [DEBUG] Looking up athlete: {athlete_key}")
        athlete_db_id = athlete_map.get(athlete_key)
        if i <= 3:
            print(f"  [DEBUG] Looked up athlete_db_id: {athlete_db_id}")
        if not athlete_db_id:
            if i <= 3:
                print(f"  ‚ö†Ô∏è  Skipping result {i} - athlete not found")
            stats['skipped_results'] += 1
            stats['skipped_missing_athlete'] += 1
            continue

        # Get meet_id from race using the race_to_meet_map
        race_meet_id = race_to_meet_map.get(result['athletic_net_race_id'])
        if i <= 3:
            print(f"  [DEBUG] Looked up race_meet_id: {race_meet_id}")
        if not race_meet_id:
            if i <= 3:
                print(f"  ‚ö†Ô∏è  Skipping result {i} - race_meet_id not found")
            stats['skipped_results'] += 1
            continue

        # Check if result already exists (unique constraint on athlete_id, meet_id, race_id, data_source)
        existing = supabase.table('results').select('id').eq('athlete_id', athlete_db_id).eq('meet_id', race_meet_id).eq('race_id', race_db_id).eq('data_source', 'athletic_net').execute()

        if i <= 3:
            print(f"  [DEBUG] Existing result check returned: {len(existing.data) if existing.data else 0} results")

        if existing.data:
            if i <= 3:
                print(f"  [DEBUG] Result {i} already exists, skipping")
            stats['skipped_results'] += 1
            stats['skipped_already_exists'] += 1
            continue

        if i <= 3:
            print(f"  [DEBUG] Adding result {i} to batch...")

        result_data = {
            'race_id': race_db_id,
            'athlete_id': athlete_db_id,
            'meet_id': race_meet_id,  # Required field
            'time_cs': int(result['time_cs']),
            'place_overall': int(result['place_overall']),
            'is_legacy_data': True,
            'data_source': 'athletic_net'  # Must be one of: excel_import, athletic_net, manual_import, scraper
        }
        results_to_import.append(result_data)

        # Batch insert when we reach BATCH_SIZE
        if len(results_to_import) >= BATCH_SIZE:
            try:
                supabase.table('results').insert(results_to_import).execute()
                stats['results_inserted'] += len(results_to_import)
                print(f"  ‚úÖ Inserted {stats['results_inserted']} results...")
            except Exception as e:
                # If batch fails due to duplicates, insert one by one
                print(f"  ‚ö†Ô∏è  Batch insert failed, inserting individually...")
                for result_data in results_to_import:
                    try:
                        supabase.table('results').insert(result_data).execute()
                        stats['results_inserted'] += 1
                    except Exception as e:
                        print(f"      ‚ùå Failed to insert result: {str(e)[:100]}")
                        stats['skipped_results'] += 1
            results_to_import = []

    # Insert remaining results
    if results_to_import:
        try:
            supabase.table('results').insert(results_to_import).execute()
            stats['results_inserted'] += len(results_to_import)
        except Exception as e:
            # If batch fails due to duplicates, insert one by one
            print(f"  ‚ö†Ô∏è  Batch insert failed, inserting individually...")
            for result_data in results_to_import:
                try:
                    supabase.table('results').insert(result_data).execute()
                    stats['results_inserted'] += 1
                except Exception as e:
                    print(f"      ‚ùå Failed to insert result: {str(e)[:100]}")
                    stats['skipped_results'] += 1

    print(f"  ‚úÖ Inserted {stats['results_inserted']} total results")
    if stats['skipped_results'] > 0:
        print(f"  ‚ö†Ô∏è  Skipped {stats['skipped_results']} results:")
        if stats['skipped_already_exists'] > 0:
            print(f"      - {stats['skipped_already_exists']} already exist")
        if stats['skipped_missing_athlete'] > 0:
            print(f"      - {stats['skipped_missing_athlete']} missing athlete")
        if stats['skipped_missing_race'] > 0:
            print(f"      - {stats['skipped_missing_race']} missing race")

    # Print final summary
    print(f"\n{'=' * 60}")
    print(f"‚úÖ Import complete!")
    print(f"\nüìä Summary:")
    print(f"  Venues: {stats['venues_created']}")
    print(f"  Courses: {stats['courses_created']}")
    print(f"  Schools: {stats['schools_created']}")
    print(f"  Athletes: {stats['athletes_created']}")
    print(f"  Meets: {stats['meets_created']}")
    print(f"  Races: {stats['races_created']}")
    print(f"  Results: {stats['results_inserted']}")

    return stats


def move_to_processed(folder_path: str):
    """
    Move imported folder to /processed/{timestamp}/

    Args:
        folder_path: Path to folder to move
    """
    timestamp = int(datetime.now().timestamp())
    processed_dir = os.path.join(os.path.dirname(folder_path), '../processed', str(timestamp))
    os.makedirs(processed_dir, exist_ok=True)

    folder_name = os.path.basename(folder_path)
    destination = os.path.join(processed_dir, folder_name)

    shutil.move(folder_path, destination)
    print(f"\nüì¶ Moved to: {destination}")


# ==============================================================================
# CLI INTERFACE
# ==============================================================================

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage:")
        print("  python import_csv_data.py <folder_path> [--preview]")
        print("\nExample:")
        print("  python import_csv_data.py to-be-processed/meet_265306_1761610508")
        print("  python import_csv_data.py to-be-processed/meet_265306_1761610508 --preview")
        sys.exit(1)

    folder_path = sys.argv[1]
    preview_only = '--preview' in sys.argv

    if not os.path.exists(folder_path):
        print(f"‚ùå Error: Folder not found: {folder_path}")
        sys.exit(1)

    # Import data
    stats = import_csv_folder(folder_path, preview_only=preview_only)

    # Move to processed if successful
    if not preview_only and stats['results_inserted'] > 0:
        move_to_processed(folder_path)
