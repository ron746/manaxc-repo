#!/usr/bin/env python3
"""
CSV Data Importer for ManaXC
Imports data from CSV files generated by athletic_net_scraper_v2.py
with validation, preview mode, and batch processing.
"""

import sys
import csv
import json
import os
import shutil
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
from supabase import create_client, Client

# Load environment variables
from dotenv import load_dotenv
load_dotenv(os.path.join(os.path.dirname(__file__), '../../website/.env.local'))

SUPABASE_URL = os.getenv('NEXT_PUBLIC_SUPABASE_URL')
SUPABASE_KEY = os.getenv('NEXT_PUBLIC_SUPABASE_ANON_KEY')

if not SUPABASE_URL or not SUPABASE_KEY:
    print("‚ùå Error: Missing Supabase credentials")
    sys.exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)


# ==============================================================================
# VALIDATION FUNCTIONS
# ==============================================================================

@dataclass
class ValidationResult:
    """Result of validation check"""
    is_valid: bool
    errors: List[str]
    warnings: List[str]


def validate_result(result: Dict, season_year: int) -> ValidationResult:
    """
    Validate a single result record.

    Critical validations (must pass):
    - Time: 60000-240000 cs (10:00-40:00)
    - Required fields: athlete_name, time_cs, place_overall, grade

    Warning validations (log but allow):
    - Unusual fast: < 80000 cs (sub-13:20)
    - Unusual slow: > 180000 cs (30:00+)
    - Grade out of range: < 9 or > 12

    Args:
        result: Result dictionary from CSV
        season_year: Season year for context

    Returns:
        ValidationResult with is_valid, errors, and warnings
    """
    errors = []
    warnings = []

    # Critical: Required fields
    if not result.get('athlete_name'):
        errors.append("Missing athlete_name")

    if not result.get('time_cs'):
        errors.append("Missing time_cs")
    else:
        time_cs = int(result['time_cs'])

        # Critical: Time range (allow up to 40 minutes for very slow finishers)
        if time_cs < 60000 or time_cs > 240000:
            errors.append(f"Invalid time: {time_cs} cs (must be 60000-240000)")

        # Warning: Unusual times
        if time_cs < 80000:
            warnings.append(f"Unusually fast time: {time_cs} cs (sub-13:20)")
        elif time_cs > 180000:
            warnings.append(f"Unusually slow time: {time_cs} cs (30:00+)")

    if not result.get('place_overall'):
        errors.append("Missing place_overall")

    if not result.get('grade'):
        errors.append("Missing grade")
    else:
        grade = int(result['grade'])
        if grade < 9 or grade > 13:
            warnings.append(f"Grade out of range: {grade} (expected 9-13)")

    return ValidationResult(
        is_valid=len(errors) == 0,
        errors=errors,
        warnings=warnings
    )


def validate_athlete(athlete: Dict, season_year: int) -> ValidationResult:
    """Validate athlete record"""
    errors = []
    warnings = []

    if not athlete.get('name'):
        errors.append("Missing athlete name")

    if not athlete.get('first_name') or not athlete.get('last_name'):
        warnings.append("Missing first or last name")

    if not athlete.get('grad_year'):
        errors.append("Missing grad_year")
    else:
        grad_year = int(athlete['grad_year'])
        expected_min = season_year - 1
        expected_max = season_year + 4
        if grad_year < expected_min or grad_year > expected_max:
            warnings.append(f"Unusual grad_year: {grad_year} (expected {expected_min}-{expected_max})")

    return ValidationResult(
        is_valid=len(errors) == 0,
        errors=errors,
        warnings=warnings
    )


# ==============================================================================
# CSV READING FUNCTIONS
# ==============================================================================

def read_csv_folder(folder_path: str) -> Dict:
    """
    Read all CSV files from a scrape folder.

    Args:
        folder_path: Path to folder containing CSV files

    Returns:
        Dictionary with all data:
        {
            'venues': [...],
            'courses': [...],
            'schools': [...],
            'athletes': [...],
            'meets': [...],
            'races': [...],
            'results': [...],
            'metadata': {...}
        }
    """
    data = {
        'venues': [],
        'courses': [],
        'schools': [],
        'athletes': [],
        'meets': [],
        'races': [],
        'results': [],
        'metadata': {}
    }

    # Read metadata
    metadata_file = os.path.join(folder_path, 'metadata.json')
    if os.path.exists(metadata_file):
        with open(metadata_file, 'r', encoding='utf-8') as f:
            data['metadata'] = json.load(f)

    # Read each CSV file
    csv_files = {
        'venues': 'venues.csv',
        'courses': 'courses.csv',
        'schools': 'schools.csv',
        'athletes': 'athletes.csv',
        'meets': 'meets.csv',
        'races': 'races.csv',
        'results': 'results.csv'
    }

    for key, filename in csv_files.items():
        filepath = os.path.join(folder_path, filename)
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                data[key] = list(reader)

    return data


# ==============================================================================
# IMPORT FUNCTIONS
# ==============================================================================

def import_csv_folder(
    folder_path: str,
    preview_only: bool = False,
    target_school_name: Optional[str] = None
) -> Dict:
    """
    Import data from CSV folder to Supabase.

    Args:
        folder_path: Path to folder with CSV files
        preview_only: If True, validate but don't import
        target_school_name: Optional school name filter

    Returns:
        Dictionary with import statistics and validation results
    """
    print(f"\nüì• {'Previewing' if preview_only else 'Importing'} data from {folder_path}")
    print("=" * 60)

    # Read CSV files
    data = read_csv_folder(folder_path)
    metadata = data['metadata']
    season_year = metadata.get('season_year', datetime.now().year)

    # Statistics
    stats = {
        'venues_created': 0,
        'courses_created': 0,
        'schools_created': 0,
        'athletes_created': 0,
        'meets_created': 0,
        'races_created': 0,
        'results_inserted': 0,
        'validation_errors': [],
        'validation_warnings': [],
        'skipped_results': 0
    }

    # Validate all results
    print(f"\nüîç Validating {len(data['results'])} results...")
    for idx, result in enumerate(data['results']):
        validation = validate_result(result, season_year)

        if not validation.is_valid:
            stats['validation_errors'].extend([f"Result {idx+1}: {e}" for e in validation.errors])

        stats['validation_warnings'].extend([f"Result {idx+1}: {w}" for w in validation.warnings])

    # Validate all athletes
    print(f"üîç Validating {len(data['athletes'])} athletes...")
    for idx, athlete in enumerate(data['athletes']):
        validation = validate_athlete(athlete, season_year)

        if not validation.is_valid:
            stats['validation_errors'].extend([f"Athlete {idx+1}: {e}" for e in validation.errors])

        stats['validation_warnings'].extend([f"Athlete {idx+1}: {w}" for w in validation.warnings])

    # Print validation summary
    print(f"\nüìä Validation Summary:")
    print(f"  Errors: {len(stats['validation_errors'])}")
    print(f"  Warnings: {len(stats['validation_warnings'])}")

    if stats['validation_errors']:
        print(f"\n‚ùå Validation Errors:")
        for error in stats['validation_errors'][:10]:  # Show first 10
            print(f"  - {error}")
        if len(stats['validation_errors']) > 10:
            print(f"  ... and {len(stats['validation_errors']) - 10} more")

    if stats['validation_warnings']:
        print(f"\n‚ö†Ô∏è  Validation Warnings:")
        for warning in stats['validation_warnings'][:5]:  # Show first 5
            print(f"  - {warning}")
        if len(stats['validation_warnings']) > 5:
            print(f"  ... and {len(stats['validation_warnings']) - 5} more")

    # Stop if preview mode or validation errors
    if preview_only:
        print(f"\n‚úÖ Preview complete - no data imported")
        return stats

    if stats['validation_errors']:
        print(f"\n‚ùå Cannot import - fix validation errors first")
        return stats

    # ========== STAGE 1: IMPORT VENUES ==========
    print(f"\nüìç Stage 1/7: Importing Venues")
    venue_id_map = {}  # Map venue_name to database ID

    for venue in data['venues']:
        # Check if exists
        existing = supabase.table('venues').select('id').eq('name', venue['name']).execute()

        if existing.data:
            venue_id_map[venue['name']] = existing.data[0]['id']
        else:
            venue_data = {
                'name': venue['name'],
                'city': venue.get('city', ''),
                'state': venue.get('state', ''),
                'athletic_net_id': venue.get('athletic_net_id'),
                'notes': venue.get('notes', '')
            }
            response = supabase.table('venues').insert(venue_data).execute()
            venue_id_map[venue['name']] = response.data[0]['id']
            stats['venues_created'] += 1
            print(f"  ‚úÖ Created venue: {venue['name']}")

    # ========== STAGE 2: IMPORT COURSES ==========
    print(f"\nüèÉ Stage 2/7: Importing Courses")
    course_id_map = {}  # Map course_name to database ID

    for course in data['courses']:
        # Get venue ID
        venue_id = venue_id_map.get(course['venue_name'])
        if not venue_id:
            print(f"  ‚ö†Ô∏è  Skipping course {course['name']} - venue not found")
            continue

        # Check if exists
        existing = supabase.table('courses').select('id').eq('name', course['name']).eq('venue_id', venue_id).execute()

        if existing.data:
            course_id_map[course['name']] = existing.data[0]['id']
        else:
            course_data = {
                'name': course['name'],
                'venue_id': venue_id,
                'distance_meters': int(course['distance_meters']),
                'difficulty_rating': float(course.get('difficulty_rating', 5.0)),
                'athletic_net_id': course.get('athletic_net_id'),
            }
            response = supabase.table('courses').insert(course_data).execute()
            course_id_map[course['name']] = response.data[0]['id']
            stats['courses_created'] += 1
            print(f"  ‚úÖ Created course: {course['name']}")

    # ========== STAGE 3: IMPORT SCHOOLS ==========
    print(f"\nüè´ Stage 3/7: Importing Schools")
    school_id_map = {}  # Map athletic_net_id to database ID

    for school in data['schools']:
        # Check if exists
        existing = supabase.table('schools').select('id').eq('name', school['name']).execute()

        if existing.data:
            school_id_map[school['athletic_net_id']] = existing.data[0]['id']
        else:
            school_data = {
                'name': school['name'],
                'short_name': school.get('short_name', school['name']),
                'city': school.get('city', ''),
                'state': school.get('state', ''),
                'athletic_net_id': school['athletic_net_id'],
            }
            response = supabase.table('schools').insert(school_data).execute()
            school_id_map[school['athletic_net_id']] = response.data[0]['id']
            stats['schools_created'] += 1
            print(f"  ‚úÖ Created school: {school['name']}")

    # ========== STAGE 4: IMPORT ATHLETES ==========
    print(f"\nüë• Stage 4/7: Importing Athletes")
    athlete_map = {}  # Map (name, school_id) to database ID

    for athlete in data['athletes']:
        school_db_id = school_id_map.get(athlete['school_athletic_net_id'])
        if not school_db_id:
            print(f"  ‚ö†Ô∏è  Skipping athlete {athlete['name']} - school not found")
            continue

        # Check if exists
        existing = supabase.table('athletes').select('id').eq('first_name', athlete['first_name']).eq('last_name', athlete['last_name']).eq('school_id', school_db_id).execute()

        if existing.data:
            athlete_map[(athlete['name'], athlete['school_athletic_net_id'])] = existing.data[0]['id']
        else:
            # Convert gender string to boolean (schema says BOOLEAN)
            gender_bool = athlete['gender'] == 'M'

            athlete_data = {
                'name': athlete['name'],  # Combined name field in current schema
                'first_name': athlete['first_name'],
                'last_name': athlete['last_name'],
                'school_id': school_db_id,
                'grad_year': int(athlete['grad_year']),
                'gender': gender_bool,  # Boolean field
                'athletic_net_id': athlete.get('athletic_net_id') or ''
            }
            response = supabase.table('athletes').insert(athlete_data).execute()
            athlete_map[(athlete['name'], athlete['school_athletic_net_id'])] = response.data[0]['id']
            stats['athletes_created'] += 1

    print(f"  ‚úÖ Created {stats['athletes_created']} athletes")

    # ========== STAGE 5: IMPORT MEETS ==========
    print(f"\nüèÅ Stage 5/7: Importing Meets")
    meet_id_map = {}  # Map athletic_net_id to database ID

    for meet in data['meets']:
        # Get venue ID (meets are at venues, not courses)
        # Use the venue from our venue_id_map
        venue_id = venue_id_map.get(meet.get('venue_name')) or venue_id_map.get('Unknown Venue')
        if not venue_id:
            print(f"  ‚ö†Ô∏è  Skipping meet {meet['name']} - no venue available")
            continue

        # Check if exists
        existing = supabase.table('meets').select('id').eq('athletic_net_id', meet['athletic_net_id']).execute()

        if existing.data:
            meet_id_map[meet['athletic_net_id']] = existing.data[0]['id']
        else:
            meet_data = {
                'name': meet['name'],
                'meet_date': meet['meet_date'],
                'venue_id': venue_id,  # Changed from course_id to venue_id
                'season_year': int(meet['season_year']),
                'athletic_net_id': meet['athletic_net_id'],
            }
            response = supabase.table('meets').insert(meet_data).execute()
            meet_id_map[meet['athletic_net_id']] = response.data[0]['id']
            stats['meets_created'] += 1
            print(f"  ‚úÖ Created meet: {meet['name']}")

    # ========== STAGE 6: IMPORT RACES ==========
    print(f"\nüèÉ‚Äç‚ôÇÔ∏è Stage 6/7: Importing Races")
    race_id_map = {}  # Map athletic_net_race_id to database ID

    for race in data['races']:
        meet_db_id = meet_id_map.get(race['meet_athletic_net_id'])
        if not meet_db_id:
            print(f"  ‚ö†Ô∏è  Skipping race - meet not found")
            continue

        # Get course_id - races are on courses
        # Use first course created (all races in this meet use same course)
        course_id = list(course_id_map.values())[0] if course_id_map else None
        if not course_id:
            print(f"  ‚ö†Ô∏è  Skipping race - no course available")
            continue

        # Convert gender string to boolean
        gender_bool = race['gender'] == 'M'

        race_data = {
            'meet_id': meet_db_id,
            'course_id': course_id,  # Races are on courses
            'name': race['name'],
            'gender': gender_bool,  # Boolean field
            'athletic_net_race_id': race['athletic_net_race_id']
        }
        response = supabase.table('races').insert(race_data).execute()
        race_id_map[race['athletic_net_race_id']] = response.data[0]['id']
        stats['races_created'] += 1

    print(f"  ‚úÖ Created {stats['races_created']} races")

    # ========== STAGE 7: IMPORT RESULTS ==========
    print(f"\nüìä Stage 7/7: Importing Results")

    # Batch import results (100 at a time)
    BATCH_SIZE = 100
    results_to_import = []

    for result in data['results']:
        race_db_id = race_id_map.get(result['athletic_net_race_id'])
        if not race_db_id:
            stats['skipped_results'] += 1
            continue

        athlete_db_id = athlete_map.get((result['athlete_name'], result['athlete_school_id']))
        if not athlete_db_id:
            stats['skipped_results'] += 1
            continue

        result_data = {
            'race_id': race_db_id,
            'athlete_id': athlete_db_id,
            'time_cs': int(result['time_cs']),
            'place_overall': int(result['place_overall']),
            'is_legacy_data': True,
            'data_source': 'athletic_net_scraper_v2'
        }
        results_to_import.append(result_data)

        # Batch insert when we reach BATCH_SIZE
        if len(results_to_import) >= BATCH_SIZE:
            supabase.table('results').insert(results_to_import).execute()
            stats['results_inserted'] += len(results_to_import)
            print(f"  ‚úÖ Inserted {stats['results_inserted']} results...")
            results_to_import = []

    # Insert remaining results
    if results_to_import:
        supabase.table('results').insert(results_to_import).execute()
        stats['results_inserted'] += len(results_to_import)

    print(f"  ‚úÖ Inserted {stats['results_inserted']} total results")
    if stats['skipped_results'] > 0:
        print(f"  ‚ö†Ô∏è  Skipped {stats['skipped_results']} results (missing athlete/race)")

    # Print final summary
    print(f"\n{'=' * 60}")
    print(f"‚úÖ Import complete!")
    print(f"\nüìä Summary:")
    print(f"  Venues: {stats['venues_created']}")
    print(f"  Courses: {stats['courses_created']}")
    print(f"  Schools: {stats['schools_created']}")
    print(f"  Athletes: {stats['athletes_created']}")
    print(f"  Meets: {stats['meets_created']}")
    print(f"  Races: {stats['races_created']}")
    print(f"  Results: {stats['results_inserted']}")

    return stats


def move_to_processed(folder_path: str):
    """
    Move imported folder to /processed/{timestamp}/

    Args:
        folder_path: Path to folder to move
    """
    timestamp = int(datetime.now().timestamp())
    processed_dir = os.path.join(os.path.dirname(folder_path), '../processed', str(timestamp))
    os.makedirs(processed_dir, exist_ok=True)

    folder_name = os.path.basename(folder_path)
    destination = os.path.join(processed_dir, folder_name)

    shutil.move(folder_path, destination)
    print(f"\nüì¶ Moved to: {destination}")


# ==============================================================================
# CLI INTERFACE
# ==============================================================================

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage:")
        print("  python import_csv_data.py <folder_path> [--preview]")
        print("\nExample:")
        print("  python import_csv_data.py to-be-processed/meet_265306_1761610508")
        print("  python import_csv_data.py to-be-processed/meet_265306_1761610508 --preview")
        sys.exit(1)

    folder_path = sys.argv[1]
    preview_only = '--preview' in sys.argv

    if not os.path.exists(folder_path):
        print(f"‚ùå Error: Folder not found: {folder_path}")
        sys.exit(1)

    # Import data
    stats = import_csv_folder(folder_path, preview_only=preview_only)

    # Move to processed if successful
    if not preview_only and stats['results_inserted'] > 0:
        move_to_processed(folder_path)
